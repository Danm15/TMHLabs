{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 3"
      ],
      "metadata": {
        "id": "1zVF2yDFx2J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_data = \"/content/drive/MyDrive/TMH/lab 3/data/\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi5DskEGv7jg",
        "outputId": "7f9d67e9-4658-4e3a-892f-da800d7f35eb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_9IjODb-QRQg",
        "outputId": "cee0400c-eed5-444b-eaed-6ee4728d0ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $path_to_data\n",
        "!wget -q http://www.gutenberg.org/files/11/11-0.txt -O \"Alice_Adventures_in_Wonderland.txt\"\n",
        "%ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2GJ0vg7-GOh",
        "outputId": "2d1208ab-d6c8-48a2-a8c4-ca4852c2e9d0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/TMH/lab 3/data\n",
            "Alice_Adventures_in_Wonderland.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim import corpora, models\n",
        "from typing import List, Tuple\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "0qjpcu4e02SX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e389bf5-26db-4b91-8daa-0b310a9bb52a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVGxG5NRlqMb",
        "outputId": "69ebf29f-5520-495f-a864-e5e2f9ab7508"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<br /><br /><br />\n",
        "# Tasks"
      ],
      "metadata": {
        "id": "5EXdG35s7kNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Завантажте дані.\n",
        "\n",
        "2. Реалізувати пайплайн опрацювання обраного тексту англійською мовою,\n",
        "включно з усією необхідною попередньою обробкою тексту, включно з\n",
        "приведенням слів до нижнього регістру, видаленням стоп-слів, цифр/неалфавітних\n",
        "символів, знаків пунктуації.\n",
        "\n",
        "3. Розділити текст на глави і в кожній главі відібрати Топ-20 слів за\n",
        "допомогою алгоритму TF-IDF.\n",
        "\n",
        "4. Реалізувати LDA алгоритм і порівняти результати з отриманими раніше за\n",
        "допомогою TF-IDF. Зробити висновки про застосовність реалізованих підходів."
      ],
      "metadata": {
        "id": "FtHt1NIaBrk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Читання тексту з файлу"
      ],
      "metadata": {
        "id": "IVwvnxs3wJsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Alice_Adventures_in_Wonderland.txt', 'r', encoding='utf-8') as file:\n",
        "    full_text = file.read()\n",
        "\n",
        "len(full_text)"
      ],
      "metadata": {
        "id": "eKy7tbrY02Pz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d578d195-0eb5-41b6-990d-5e7c6fc6f709"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164047"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Пайплайн опрацювання тексту"
      ],
      "metadata": {
        "id": "xcVoaWcnvzKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = full_text\n",
        "\n",
        "start_indicator = \"\"\"\n",
        "CHAPTER I.\n",
        "Down the Rabbit-Hole\n",
        "\"\"\"\n",
        "start_idx = text.find(start_indicator)\n",
        "assert start_idx != -1\n",
        "text = text[start_idx : ]\n",
        "\n",
        "end_indicator = \"\"\"\n",
        "THE END\"\"\"\n",
        "end_idx = text.find(end_indicator)\n",
        "assert end_idx != -1\n",
        "text = text[ : end_idx]\n",
        "\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OyzvAxVyGp1",
        "outputId": "572df736-c6b1-41fa-f77f-49ce931a0f29"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144009"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Розділити текст на глави\n",
        "\n",
        "(робимо це перед обробкою тексту, бо після неї плейсхолдери глав можуть змінитися)\n"
      ],
      "metadata": {
        "id": "TckLwMd7zX9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chapter_indices = [(match.start(), match.end()) for match in re.finditer('CHAPTER [IVXLCDM]+.\\n', text)]\n",
        "chapter_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ajmvxxyyB6Z",
        "outputId": "29a008eb-4ac0-4125-af5b-9d388d66c053"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 12),\n",
              " (11557, 11569),\n",
              " (22515, 22528),\n",
              " (31781, 31793),\n",
              " (45670, 45681),\n",
              " (57686, 57698),\n",
              " (71535, 71548),\n",
              " (84244, 84258),\n",
              " (97919, 97931),\n",
              " (110555, 110566),\n",
              " (121971, 121983),\n",
              " (132363, 132376)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def divide_into_chapters(text: str, chapter_indices: List[Tuple[int]]):\n",
        "    chapters = []\n",
        "    for ((start_a, start_b), (end_a, end_b)) in zip(chapter_indices, chapter_indices[1:] + [(None, None)]):\n",
        "        chapters.append(text[start_b : end_a])\n",
        "    return chapters\n",
        "\n",
        "chapters = divide_into_chapters(text, chapter_indices)\n",
        "len(chapters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJCCdb0IyD1J",
        "outputId": "84ce7d89-77d3-4c66-e24d-d558da314920"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Попередня обробка, включно з:\n",
        "- приведенням слів до нижнього регістру,\n",
        "- видаленням стоп-слів,\n",
        "- цифр/неалфавітних символів,\n",
        "- знаків пунктуації."
      ],
      "metadata": {
        "id": "pMW-hbYMzZTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_chapters = []\n",
        "for chapter in chapters:\n",
        "\n",
        "    chapter = chapter.lower()\n",
        "\n",
        "    chapter = re.sub(r'[^a-zA-Z\\s]', '', chapter)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(chapter)\n",
        "    filtered_text = [word for word in words if word.lower() not in stop_words]\n",
        "    processed_chapters.append(' '.join(filtered_text))\n",
        "\n",
        "len(processed_chapters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2mfQ5UIwXuY",
        "outputId": "1ab74ccf-0238-4bda-d29d-4e38f0874015"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1nEILXNoxRVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. В кожній главі відібрати Топ-20 слів за допомогою алгоритму TF-IDF"
      ],
      "metadata": {
        "id": "8IzT7N92wV0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=20)\n",
        "\n",
        "for i, chapter in enumerate(processed_chapters):\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform([chapter])\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    top_words_index = np.argsort(tfidf_matrix.toarray().flatten())[::-1][:20]\n",
        "    top_words = [feature_names[j] for j in top_words_index]\n",
        "    print(f\"\\nTop words in CHAPTER {i + 1}:\\n{', '.join(top_words)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZS6e6j2u-WZ",
        "outputId": "f47fdb2f-1af7-419d-aa8a-ba428fa72b91"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top words in CHAPTER 1:\n",
            "alice, little, way, think, like, see, one, could, time, thought, said, found, door, eat, would, get, went, well, nothing, either\n",
            "\n",
            "Top words in CHAPTER 2:\n",
            "alice, little, mouse, im, said, dear, go, went, must, oh, thought, like, way, pool, feet, cried, one, ill, could, cats\n",
            "\n",
            "Top words in CHAPTER 3:\n",
            "said, alice, mouse, dodo, know, soon, one, round, dry, long, lory, thing, would, must, little, prizes, ill, question, quite, course\n",
            "\n",
            "Top words in CHAPTER 4:\n",
            "alice, little, said, one, rabbit, thought, bill, sure, get, heard, came, window, made, quite, thing, like, went, great, room, moment\n",
            "\n",
            "Top words in CHAPTER 5:\n",
            "said, alice, caterpillar, im, pigeon, little, well, serpent, ive, think, one, know, see, youre, last, tried, got, dont, bit, youth\n",
            "\n",
            "Top words in CHAPTER 6:\n",
            "said, alice, cat, like, little, duchess, much, baby, would, mad, went, know, footman, could, thought, get, see, quite, door, large\n",
            "\n",
            "Top words in CHAPTER 7:\n",
            "said, alice, hatter, dormouse, hare, march, time, know, well, one, went, say, little, thing, twinkle, tea, think, dont, replied, table\n",
            "\n",
            "Top words in CHAPTER 8:\n",
            "said, alice, queen, head, king, three, one, went, see, soldiers, like, two, cat, came, began, looked, gardeners, dont, could, away\n",
            "\n",
            "Top words in CHAPTER 9:\n",
            "said, alice, turtle, mock, gryphon, duchess, queen, went, never, little, dont, say, know, quite, moral, like, day, thought, come, much\n",
            "\n",
            "Top words in CHAPTER 10:\n",
            "said, gryphon, alice, turtle, mock, would, dance, soup, beautiful, voice, wont, join, like, know, whiting, could, replied, lobsters, first, lobster\n",
            "\n",
            "Top words in CHAPTER 11:\n",
            "said, king, hatter, alice, court, dormouse, one, witness, queen, rabbit, began, white, im, thought, next, voice, march, made, see, breadandbutter\n",
            "\n",
            "Top words in CHAPTER 12:\n",
            "said, alice, king, would, little, jury, queen, know, rabbit, one, white, nothing, head, gave, could, sister, theres, voice, dream, must\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UVCAD5_7u-UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Реалізувати LDA алгоритм, <br /> порівняти результати з отриманими раніше за допомогою TF-IDF. <br /> Зробити висновки про застосовність реалізованих підходів."
      ],
      "metadata": {
        "id": "o3b1xVTpwk0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "texts = [word_tokenize(chapter) for chapter in processed_chapters]\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
        "\n",
        "\n",
        "for i in range(lda_model.num_topics):\n",
        "    print(f\"\\nLDA Topic {i + 1}:\\n{lda_model.print_topic(i)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uiLjiv3wksr",
        "outputId": "b129d5b4-c036-46df-a27b-4c718aa16394"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LDA Topic 1:\n",
            "0.001*\"said\" + 0.001*\"alice\" + 0.000*\"little\" + 0.000*\"caterpillar\" + 0.000*\"one\" + 0.000*\"well\" + 0.000*\"time\" + 0.000*\"know\" + 0.000*\"im\" + 0.000*\"think\"\n",
            "\n",
            "LDA Topic 2:\n",
            "0.024*\"alice\" + 0.018*\"said\" + 0.014*\"little\" + 0.008*\"one\" + 0.007*\"im\" + 0.007*\"like\" + 0.007*\"thought\" + 0.006*\"see\" + 0.005*\"caterpillar\" + 0.005*\"went\"\n",
            "\n",
            "LDA Topic 3:\n",
            "0.039*\"said\" + 0.026*\"alice\" + 0.021*\"hatter\" + 0.015*\"dormouse\" + 0.011*\"march\" + 0.010*\"king\" + 0.010*\"hare\" + 0.009*\"one\" + 0.008*\"time\" + 0.007*\"know\"\n",
            "\n",
            "LDA Topic 4:\n",
            "0.038*\"said\" + 0.027*\"alice\" + 0.009*\"would\" + 0.008*\"queen\" + 0.007*\"know\" + 0.007*\"little\" + 0.007*\"like\" + 0.006*\"one\" + 0.006*\"could\" + 0.006*\"gryphon\"\n",
            "\n",
            "LDA Topic 5:\n",
            "0.035*\"said\" + 0.029*\"alice\" + 0.016*\"mock\" + 0.016*\"turtle\" + 0.012*\"gryphon\" + 0.011*\"duchess\" + 0.008*\"went\" + 0.008*\"queen\" + 0.006*\"little\" + 0.006*\"dont\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6r-RdFubIoFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}